# Scarlet AI ğŸ¤–âœ¨

Scarlet is a **privacy-aware, emotionally intelligent, voice-first AI assistant framework** designed to run on both **high-performance systems (Raspberry Pi 5 / PC)** and **low-resource devices (Raspberry Pi Zero 2 W)**.

This repository provides a **sanitized, demo-friendly version** of Scarlet intended for **learning, showcasing architecture, and portfolio/interview purposes**, while keeping sensitive or personal implementations private.

---

## ğŸŒŸ Vision

Scarlet is not just a chatbot, but she is personal companion.

She is designed as:

* A **human-like conversational assistant**
* A **voice-first AI companion**
* A **modular AI platform** for experimentation
* A **safe, controllable, and explainable AI system**

Primary long-term goals include:

* Elder-care and hospital assistance
* Hands-free home AI
* Emotion-aware interactions
* Safe semi-autonomous behaviors

---

## ğŸ§  Core Concepts

Scarlet is built around the following principles:

* **Voice First** â€“ Wake-word, VAD-based listening, TTS responses
* **Session Awareness** â€“ Conversations persist without repeated wake-words
* **Role-Based Behavior** â€“ Creator / Known User / Guest logic
* **Emotion & Tone Modeling** â€“ Responses adapt to context and user
* **Safety First** â€“ Explicit rules, filters, and kill-switch design
* **Hardware-Aware** â€“ Same architecture scales from Pi Zero to Pi 5

---

## ğŸ§© Projects in the Scarlet Ecosystem

### 1ï¸âƒ£ Scarlet (Main)

High-capability version designed for:

* Raspberry Pi 5 / PC
* Offline + Online hybrid usage
* Rich TTS (Piper / Google TTS)
* Advanced VAD + Faster-Whisper STT
* Emotion videos / expressions

### 2ï¸âƒ£ Jasmine (Mini Scarlet)

Lightweight online-only version designed for:

* Raspberry Pi Zero 2 W
* Low memory and CPU
* Groq-hosted Whisper + LLM
* gTTS / lightweight TTS

> This repository primarily focuses on **Scarlet Core Architecture**, with references to Jasmine where relevant.

---

## ğŸ—ï¸ High-Level Architecture

```
Wake Word Detection
        â†“
Face / User Identification (Optional)
        â†“
Conversation Session Manager
        â†“
Voice Activity Detection (Silero VAD)
        â†“
Speech-to-Text (Faster-Whisper / Groq Whisper)
        â†“
Intent & Behavior Routing
        â†“
LLM (Groq / Mistral / Mock)
        â†“
Response Styling & Emotion Layer
        â†“
Text-to-Speech (Piper / gTTS / Google TTS)
```

---

## ğŸ“ Repository Structure (Sanitized)

```
scarlet/
â”œâ”€â”€ main.py                 # Entry point
â”œâ”€â”€ conversation_mode.py    # VAD-based conversation loop
â”œâ”€â”€ handler.py              # Intent + role-based routing
â”œâ”€â”€ tts.py                  # Text-to-Speech abstraction
â”œâ”€â”€ wake_word.py            # Wake word logic (stub/demo)
â”œâ”€â”€ memory_mood.py          # Mood & affect tracking
â”œâ”€â”€ persona/
â”‚   â”œâ”€â”€ persona.yaml        # Assistant personality (sanitized)
â”‚   â””â”€â”€ rules.yaml          # Behavior & safety rules
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ utils_time.py       # Date & time handling
â”‚   â””â”€â”€ audio_utils.py
â”œâ”€â”€ demo/
â”‚   â”œâ”€â”€ mock_llm.py         # Mock LLM for demo mode
â”‚   â””â”€â”€ mock_tts.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

> âš ï¸ **Note:** Some internal files (face recognition, private scripts, expression assets) are intentionally excluded.

---

## ğŸ”Š Speech Stack

### ğŸ™ï¸ Input (STT)

* **Faster-Whisper** (local, high accuracy)
* **Groq Whisper API** (online, low latency)

### ğŸ—£ï¸ Output (TTS)

* Piper (offline, natural voice)
* gTTS (lightweight)
* Google Cloud TTS (neural voices â€“ optional)

---

## ğŸ§  LLM Integration

Scarlet supports pluggable LLM backends:

* **Groq API** (Ultra-fast inference)
* **Mistral 7B** (local / self-hosted)
* **Mock LLM** (demo & testing)

LLMs are **never allowed to act directly** â€” all actions pass through:

* Rule filters
* Role checks
* Safety guards

---

## ğŸ­ Personality & Behavior

Behavior is **configuration-driven**, not hardcoded.

### Role Examples:

* **Creator** â€“ Full access
* **Known User** â€“ Helpful, respectful
* **Guest** â€“ Restricted, safe defaults

### Features:

* Emotional tone mirroring
* Respectful boundaries
* No disclosure of private/internal data
* Context-aware response shaping

---

## ğŸ›¡ï¸ Safety & Control Model

Scarlet is designed with **explicit safety layers**:

* Kill-switch architecture
* Action caps
* Two-step confirmations
* Wake-word & face-gated commands
* Sensor consent rules
* Tool isolation
* Audit-friendly logs

This makes Scarlet suitable for **real-world environments**, not just demos.

---


```
Demo mode uses:

* Mock LLM
* Mock TTS
* Keyboard input fallback

---

## ğŸ§ª Who Is This Repo For?

* AI / ML Engineers
* Voice Assistant Developers
* Embedded AI enthusiasts
* Recruiters & Interview Panels
* System architects

---

## ğŸ“Œ What This Repo Is NOT

* âŒ A consumer-ready assistant
* âŒ A fully autonomous AI
* âŒ A jailbreak / uncensored LLM project

This is a **responsible AI system design project**.

---

## ğŸ§­ Roadmap (Public)

* [ ] Barge-in during TTS
* [ ] Unified SSML emotion profiles
* [ ] LED / hardware feedback hooks
* [ ] Better memory TTL handling
* [ ] Plugin-based skills

---

## ğŸ‘¤ Author

**Naga**
AI Engineer | Middleware Architect | Voice AI Systems

> Built as a solo project with a focus on learning, safety, and real-world usability.

---

## ğŸ“œ License

This project is released under the **MIT License** (for the sanitized/demo code only).

Private extensions, datasets, and personal configurations are **not included**.

---

## â­ Final Note

Scarlet represents a **bridge between classic software engineering and modern AI systems** â€” combining discipline, safety, and creativity.

If you are reviewing this repo as part of an interview or evaluation, please treat it as a **system design showcase**, not just a codebase.
